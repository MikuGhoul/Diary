## fork 源码分析

* 基于 Linux 2.6.39 内核

#### 流程简单描述
1. 父进程调用fork，由于是系统调用，会产生int软中断，陷入内核
2. 内核根据系统调用号（存储在sys_call_table中），调用 sys_fork 系统调用
3. sys_fork 调用 do_fork，do_fork 是 fork 的主体，先进行一些错误检查和准备工作，然后调用 copy_process
4. copy_process 是进行父进程复制的主体，进行一些错误检查后会调用 dup_task_struct 函数
5. dup_task_struct 
5. copy_thread 复制父进程的执行状态，包括相关寄存器的值，指令指针和建立相关的栈

#### 源码分析
1. sys_fork
    ``` c
    int sys_fork(struct pt_regs *regs)
    { 
        return do_fork(SIGCHLD, regs->sp, regs, 0, NULL, NULL);
    }
    ```
2. do_fork
    ``` c
    /*
    * clone_flags 控制复制父进程的哪些资源
    * stack_start 表示用户态下，栈的起始地址
    * regs 表示指向寄存器集合的指针
    * stack_size 用户态栈大小
    * parent_tidptr 用户态父进程tid指针
    * child_tidptr 用户态子进程tid指针
    */
    long do_fork(unsigned long clone_flags,
            unsigned long stack_start,
            struct pt_regs *regs,
            unsigned long stack_size,
            int __user *parent_tidptr,
            int __user *child_tidptr)
    {
        struct task_struct *p;
        int trace = 0;
        long nr;

        /*
        * Do some preliminary argument and permissions checking before we
        * actually start allocating stuff
        * 检查clone_flags有效性
        */
        if (clone_flags & CLONE_NEWUSER) {
            if (clone_flags & CLONE_THREAD)
                return -EINVAL;
            /* hopefully this check will go away when userns support is
            * complete
            */
            if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) ||
                    !capable(CAP_SETGID))
                return -EPERM;
        }

        /*
        * When called from kernel_thread, don't do user tracing stuff.
        */
        if (likely(user_mode(regs)))
            trace = tracehook_prepare_clone(clone_flags);

        /*
        * 复制进程描述符(Linux中的 PCB: task_struct)
        */
        p = copy_process(clone_flags, stack_start, regs, stack_size,
                child_tidptr, NULL, trace);
        /*
        * Do this prior waking up the new thread - the thread pointer
        * might get invalid after that point, if the thread exits quickly.
        */
        if (!IS_ERR(p)) {
            struct completion vfork;

            trace_sched_process_fork(current, p);

            /* 获取子进程pid */
            nr = task_pid_vnr(p);

            if (clone_flags & CLONE_PARENT_SETTID)
                put_user(nr, parent_tidptr);

            if (clone_flags & CLONE_VFORK) {
                p->vfork_done = &vfork;
                init_completion(&vfork);
            }

            audit_finish_fork(p);
            tracehook_report_clone(regs, clone_flags, nr, p);

            /*
            * We set PF_STARTING at creation in case tracing wants to
            * use this to distinguish a fully live task from one that
            * hasn't gotten to tracehook_report_clone() yet.  Now we
            * clear it and set the child going.
            */
            p->flags &= ~PF_STARTING;

            /* 唤醒子进程！！！！！！！！！！ */
            wake_up_new_task(p, clone_flags);

            tracehook_report_clone_complete(trace, regs,
                            clone_flags, nr, p);

            if (clone_flags & CLONE_VFORK) {
                freezer_do_not_count();
                wait_for_completion(&vfork);
                freezer_count();
                tracehook_report_vfork_done(p, nr);
            }
        } else {
            nr = PTR_ERR(p);
        }
        /* 返回子进程pid，至于子进程的返回值，和copy_process里的copy_thread有关，明天填坑 */
        return nr;
    }
    ```

3. copy_process
    ``` c
    static struct task_struct *copy_process(unsigned long clone_flags,
                        unsigned long stack_start,
                        struct pt_regs *regs,
                        unsigned long stack_size,
                        int __user *child_tidptr,
                        struct pid *pid,
                        int trace)
    {
        int retval;
        struct task_struct *p;
        int cgroup_callbacks_done = 0;
        
        /*
        * 检查clone_flags有效性
        */
        if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
            return ERR_PTR(-EINVAL);

        /*
        * Thread groups must share signals as well, and detached threads
        * can only be started up within the thread group.
        */
        if ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))
            return ERR_PTR(-EINVAL);

        /*
        * Shared signal handlers imply shared VM. By way of the above,
        * thread groups also imply shared VM. Blocking this case allows
        * for various simplifications in other code.
        */
        if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
            return ERR_PTR(-EINVAL);

        /*
        * Siblings of global init remain as zombies on exit since they are
        * not reaped by their parent (swapper). To solve this and to avoid
        * multi-rooted process trees, prevent global and container-inits
        * from creating siblings.
        */
        if ((clone_flags & CLONE_PARENT) &&
                    current->signal->flags & SIGNAL_UNKILLABLE)
            return ERR_PTR(-EINVAL);

        retval = security_task_create(clone_flags);
        if (retval)
            goto fork_out;

        retval = -ENOMEM;

        /*
        * 为新进程创建新的内核堆栈，thread_info 和 task_struct 结构
        * 返回子进程的 task_struct 给 p
        */
        p = dup_task_struct(current);
        if (!p)
            goto fork_out;

        ftrace_graph_init_task(p);

        /* 初始化mutex */
        rt_mutex_init_task(p);

    #ifdef CONFIG_PROVE_LOCKING
        DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
        DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
    #endif
        retval = -EAGAIN;

        /* 检查进程创建后是否超出进程上限 */
        if (atomic_read(&p->real_cred->user->processes) >=
                task_rlimit(p, RLIMIT_NPROC)) {
            if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
                p->real_cred->user != INIT_USER)
                goto bad_fork_free;
        }

        /* 拷贝父进程的信号 */
        retval = copy_creds(p, clone_flags);
        if (retval < 0)
            goto bad_fork_free;

        /*
        * If multiple threads are within copy_process(), then this check
        * triggers too late. This doesn't hurt, the check is only there
        * to stop root fork bombs.
        */
        retval = -EAGAIN;
        if (nr_threads >= max_threads)
            goto bad_fork_cleanup_count;

        if (!try_module_get(task_thread_info(p)->exec_domain->module))
            goto bad_fork_cleanup_count;

        p->did_exec = 0;
        delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */

        /* 更新子进程的flag */
        copy_flags(clone_flags, p);

        /* 初始化子进程的children和sibling链表 */
        INIT_LIST_HEAD(&p->children);
        INIT_LIST_HEAD(&p->sibling);

        rcu_copy_process(p);
        p->vfork_done = NULL;

        /* 初始化自旋锁 */
        spin_lock_init(&p->alloc_lock);

        /* 清除挂起（未决）的信号 */
        init_sigpending(&p->pending);

        /* 初始化子进程的CPU时间 */
        p->utime = cputime_zero;
        p->stime = cputime_zero;
        p->gtime = cputime_zero;
        p->utimescaled = cputime_zero;
        p->stimescaled = cputime_zero;
    #ifndef CONFIG_VIRT_CPU_ACCOUNTING
        p->prev_utime = cputime_zero;
        p->prev_stime = cputime_zero;
    #endif
    #if defined(SPLIT_RSS_COUNTING)
        memset(&p->rss_stat, 0, sizeof(p->rss_stat));
    #endif

        p->default_timer_slack_ns = current->timer_slack_ns;

        task_io_accounting_init(&p->ioac);
        acct_clear_integrals(p);

        posix_cpu_timers_init(p);

        p->lock_depth = -1;		/* -1 = no lock */
        do_posix_clock_monotonic_gettime(&p->start_time);
        p->real_start_time = p->start_time;
        monotonic_to_bootbased(&p->real_start_time);
        p->io_context = NULL;
        p->audit_context = NULL;
        cgroup_fork(p);
    #ifdef CONFIG_NUMA
        p->mempolicy = mpol_dup(p->mempolicy);
        if (IS_ERR(p->mempolicy)) {
            retval = PTR_ERR(p->mempolicy);
            p->mempolicy = NULL;
            goto bad_fork_cleanup_cgroup;
        }
        mpol_fix_fork_child_flag(p);
    #endif
    #ifdef CONFIG_TRACE_IRQFLAGS
        p->irq_events = 0;
    #ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
        p->hardirqs_enabled = 1;
    #else
        p->hardirqs_enabled = 0;
    #endif
        p->hardirq_enable_ip = 0;
        p->hardirq_enable_event = 0;
        p->hardirq_disable_ip = _THIS_IP_;
        p->hardirq_disable_event = 0;
        p->softirqs_enabled = 1;
        p->softirq_enable_ip = _THIS_IP_;
        p->softirq_enable_event = 0;
        p->softirq_disable_ip = 0;
        p->softirq_disable_event = 0;
        p->hardirq_context = 0;
        p->softirq_context = 0;
    #endif
    #ifdef CONFIG_LOCKDEP
        p->lockdep_depth = 0; /* no locks held yet */
        p->curr_chain_key = 0;
        p->lockdep_recursion = 0;
    #endif

    #ifdef CONFIG_DEBUG_MUTEXES
        p->blocked_on = NULL; /* not blocked yet */
    #endif
    #ifdef CONFIG_CGROUP_MEM_RES_CTLR
        p->memcg_batch.do_batch = 0;
        p->memcg_batch.memcg = NULL;
    #endif

        /* 执行调度器相关设置，将子进程分配给一个CPU */
        /* Perform scheduler related setup. Assign this task to a CPU. */
        sched_fork(p, clone_flags);

        retval = perf_event_init_task(p);
        if (retval)
            goto bad_fork_cleanup_policy;

        if ((retval = audit_alloc(p)))
            goto bad_fork_cleanup_policy;
            
            
        /* 拷贝所有的进程信息
         * 打开的文件，文件信息，信号处理函数，进程地址空间等等
         * 这占了fork的很大一部分 
         * 比较重要的是copy_mm时有COW机制
         */
        /* copy all the process information */
        if ((retval = copy_semundo(clone_flags, p)))
            goto bad_fork_cleanup_audit;
        if ((retval = copy_files(clone_flags, p)))
            goto bad_fork_cleanup_semundo;
        if ((retval = copy_fs(clone_flags, p)))
            goto bad_fork_cleanup_files;
        if ((retval = copy_sighand(clone_flags, p)))
            goto bad_fork_cleanup_fs;
        if ((retval = copy_signal(clone_flags, p)))
            goto bad_fork_cleanup_sighand;
        if ((retval = copy_mm(clone_flags, p)))
            goto bad_fork_cleanup_signal;
        if ((retval = copy_namespaces(clone_flags, p)))
            goto bad_fork_cleanup_mm;
        if ((retval = copy_io(clone_flags, p)))
            goto bad_fork_cleanup_namespaces;
        retval = copy_thread(clone_flags, stack_start, stack_size, p, regs);
        if (retval)
            goto bad_fork_cleanup_io;

        if (pid != &init_struct_pid) {
            retval = -ENOMEM;
            /* 申请一个pid */
            pid = alloc_pid(p->nsproxy->pid_ns);
            if (!pid)
                goto bad_fork_cleanup_io;
        }

        /* 更新子进程的pid */
        p->pid = pid_nr(pid);
        p->tgid = p->pid;
        if (clone_flags & CLONE_THREAD)
            p->tgid = current->tgid;

        if (current->nsproxy != p->nsproxy) {
            retval = ns_cgroup_clone(p, pid);
            if (retval)
                goto bad_fork_free_pid;
        }

        p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
        /*
        * Clear TID on mm_release()?
        */
        p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
    #ifdef CONFIG_BLOCK
        p->plug = NULL;
    #endif
    #ifdef CONFIG_FUTEX
        p->robust_list = NULL;
    #ifdef CONFIG_COMPAT
        p->compat_robust_list = NULL;
    #endif
        INIT_LIST_HEAD(&p->pi_state_list);
        p->pi_state_cache = NULL;
    #endif
        /*
        * sigaltstack should be cleared when sharing the same VM
        */
        if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
            p->sas_ss_sp = p->sas_ss_size = 0;

        /*
        * Syscall tracing and stepping should be turned off in the
        * child regardless of CLONE_PTRACE.
        */
        user_disable_single_step(p);
        clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
    #ifdef TIF_SYSCALL_EMU
        clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
    #endif
        clear_all_latency_tracing(p);

        /* ok, now we should be set up.. */
        p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
        p->pdeath_signal = 0;
        p->exit_state = 0;

        /*
        * Ok, make it visible to the rest of the system.
        * We dont wake it up yet.
        */
        p->group_leader = p;
        INIT_LIST_HEAD(&p->thread_group);

        /* Now that the task is set up, run cgroup callbacks if
        * necessary. We need to run them before the task is visible
        * on the tasklist. */
        cgroup_fork_callbacks(p);
        cgroup_callbacks_done = 1;

        /* Need tasklist lock for parent etc handling! */
        write_lock_irq(&tasklist_lock);

        /* 更新子进程 task_struct 中的父进程成员变量 */
        /* CLONE_PARENT re-uses the old parent */
        if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
            p->real_parent = current->real_parent;
            p->parent_exec_id = current->parent_exec_id;
        } else {
            p->real_parent = current;
            p->parent_exec_id = current->self_exec_id;
        }

        spin_lock(&current->sighand->siglock);

        /*
        * Process group and session signals need to be delivered to just the
        * parent before the fork or both the parent and the child after the
        * fork. Restart if a signal comes in before we add the new process to
        * it's process group.
        * A fatal signal pending means that current will exit, so the new
        * thread can't slip out of an OOM kill (or normal SIGKILL).
        */
        recalc_sigpending();
        if (signal_pending(current)) {
            spin_unlock(&current->sighand->siglock);
            write_unlock_irq(&tasklist_lock);
            retval = -ERESTARTNOINTR;
            goto bad_fork_free_pid;
        }

        if (clone_flags & CLONE_THREAD) {
            current->signal->nr_threads++;
            atomic_inc(&current->signal->live);
            atomic_inc(&current->signal->sigcnt);
            p->group_leader = current->group_leader;
            list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
        }

        if (likely(p->pid)) {
            tracehook_finish_clone(p, clone_flags, trace);

            if (thread_group_leader(p)) {
                if (is_child_reaper(pid))
                    p->nsproxy->pid_ns->child_reaper = p;

                p->signal->leader_pid = pid;
                p->signal->tty = tty_kref_get(current->signal->tty);
                attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
                attach_pid(p, PIDTYPE_SID, task_session(current));
                list_add_tail(&p->sibling, &p->real_parent->children);
                list_add_tail_rcu(&p->tasks, &init_task.tasks);
                __this_cpu_inc(process_counts);
            }
            attach_pid(p, PIDTYPE_PID, pid);
            nr_threads++;
        }

        total_forks++;
        spin_unlock(&current->sighand->siglock);
        write_unlock_irq(&tasklist_lock);
        proc_fork_connector(p);
        cgroup_post_fork(p);
        perf_event_fork(p);

        /* return !!! */
        return p;

    bad_fork_free_pid:
        if (pid != &init_struct_pid)
            free_pid(pid);
    bad_fork_cleanup_io:
        if (p->io_context)
            exit_io_context(p);
    bad_fork_cleanup_namespaces:
        exit_task_namespaces(p);
    bad_fork_cleanup_mm:
        if (p->mm) {
            task_lock(p);
            if (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
                atomic_dec(&p->mm->oom_disable_count);
            task_unlock(p);
            mmput(p->mm);
        }
    bad_fork_cleanup_signal:
        if (!(clone_flags & CLONE_THREAD))
            free_signal_struct(p->signal);
    bad_fork_cleanup_sighand:
        __cleanup_sighand(p->sighand);
    bad_fork_cleanup_fs:
        exit_fs(p); /* blocking */
    bad_fork_cleanup_files:
        exit_files(p); /* blocking */
    bad_fork_cleanup_semundo:
        exit_sem(p);
    bad_fork_cleanup_audit:
        audit_free(p);
    bad_fork_cleanup_policy:
        perf_event_free_task(p);
    #ifdef CONFIG_NUMA
        mpol_put(p->mempolicy);
    bad_fork_cleanup_cgroup:
    #endif
        cgroup_exit(p, cgroup_callbacks_done);
        delayacct_tsk_free(p);
        module_put(task_thread_info(p)->exec_domain->module);
    bad_fork_cleanup_count:
        atomic_dec(&p->cred->user->processes);
        exit_creds(p);
    bad_fork_free:
        free_task(p);
    fork_out:
        return ERR_PTR(retval);
    }
    ```
4. dup_task_struct
    ``` c
    static struct task_struct *dup_task_struct(struct task_struct *orig)
    {
        struct task_struct *tsk;
        struct thread_info *ti;
        unsigned long *stackend;
        int node = tsk_fork_get_node(orig);
        int err;

        prepare_to_copy(orig);

        /*
         * 在高速缓存上分配出一个task_struct节点
         * alloc_task_struct_node 调用 kmem_cache_alloc_node，会从SLAB中分配内存
         */
        tsk = alloc_task_struct_node(node);
        if (!tsk)
            return NULL;
        /*
         * 分配thread_info节点
         */        
        ti = alloc_thread_info_node(tsk, node);
        if (!ti) {
            free_task_struct(tsk);
            return NULL;
        }
        /*
         * 将父进程的信息复制到子进程的task_struct中
         */  
        err = arch_dup_task_struct(tsk, orig);
        if (err)
            goto out;
        /*
         * 将thread_info赋值给当前新创建的task
         */ 
        tsk->stack = ti;

        err = prop_local_init_single(&tsk->dirties);
        if (err)
            goto out;

        setup_thread_stack(tsk, orig);
        clear_user_return_notifier(tsk);
        clear_tsk_need_resched(tsk);
        stackend = end_of_stack(tsk);
        *stackend = STACK_END_MAGIC;	/* for overflow detection */

    #ifdef CONFIG_CC_STACKPROTECTOR
        tsk->stack_canary = get_random_int();
    #endif

        /* One for us, one for whoever does the "release_task()" (usually parent) */
        /* 改变一些子进程task_struct的成员变量，使其与父进程不同 */
        /* 修改引用计数 */
        atomic_set(&tsk->usage,2);
        /* 修改互斥资源 */
        atomic_set(&tsk->fs_excl, 0);
    #ifdef CONFIG_BLK_DEV_IO_TRACE
        /* 修改块设备IO层的跟踪工具 */
        tsk->btrace_seq = 0;
    #endif
        /* 修改管道 */
        tsk->splice_pipe = NULL;

        account_kernel_stack(ti, 1);

        /* 返回子进程的task_struct */
        return tsk;

    out:
        free_thread_info(ti);
        free_task_struct(tsk);
        return NULL;
    }
    ```

5. sched_fork
    ``` c
    void sched_fork(struct task_struct *p, int clone_flags)
    {
        int cpu = get_cpu();

        __sched_fork(p);
        /* 设置子进程状态为 TASK_RUNNING，保证其不会被真正运行，信号和其他事件不能把其唤醒并加入运行队列 */
        /*
        * We mark the process as running here. This guarantees that
        * nobody will actually run it, and a signal or other external
        * event cannot wake it up and insert it on the runqueue either.
        */
        p->state = TASK_RUNNING;

        /*
        * Revert to default priority/policy on fork if requested.
        */
        if (unlikely(p->sched_reset_on_fork)) {
            if (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {
                p->policy = SCHED_NORMAL;
                p->normal_prio = p->static_prio;
            }

            if (PRIO_TO_NICE(p->static_prio) < 0) {
                p->static_prio = NICE_TO_PRIO(0);
                p->normal_prio = p->static_prio;
                set_load_weight(p);
            }

            /*
            * We don't need the reset flag anymore after the fork. It has
            * fulfilled its duty:
            */
            p->sched_reset_on_fork = 0;
        }

        /*
        * Make sure we do not leak PI boosting priority to the child.
        */
        p->prio = current->normal_prio;

        if (!rt_prio(p->prio))
            p->sched_class = &fair_sched_class;

        if (p->sched_class->task_fork)
            p->sched_class->task_fork(p);

        /*
        * The child is not yet in the pid-hash so no cgroup attach races,
        * and the cgroup is pinned to this child due to cgroup_fork()
        * is ran before sched_fork().
        *
        * Silence PROVE_RCU.
        */
        rcu_read_lock();
        /* 为进程分配相应的CPU */
        set_task_cpu(p, cpu);
        rcu_read_unlock();

    #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
        if (likely(sched_info_on()))
            memset(&p->sched_info, 0, sizeof(p->sched_info));
    #endif
    #if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
        p->oncpu = 0;
    #endif
    #ifdef CONFIG_PREEMPT
        /* Want to start with kernel preemption disabled. */
        task_thread_info(p)->preempt_count = 1;
    #endif
    #ifdef CONFIG_SMP
        plist_node_init(&p->pushable_tasks, MAX_PRIO);
    #endif

        put_cpu();
    }
    ```

#### Reference
* [Linux进程管理(二)--fork](http://gityuan.com/2017/08/05/linux-process-fork/)

* [Linux中fork系统调用分析](http://blog.tonychow.me/syscall-fork-in-linux.html)

* [Linux内核 fork 源码分析](https://blog.csdn.net/yangbodong22011/article/details/78648419)